
# microgpt
**2026년 2월 12일**

이 글은 저의 새로운 예술 프로젝트인 **microgpt**에 대한 간략한 가이드입니다. 이 프로젝트는 의존성(dependency) 없이 오직 200줄의 순수 파이썬(Python) 코드만으로 이루어진 단일 파일로, GPT를 학습하고 추론까지 수행합니다. 이 파일에는 필요한 모든 알고리즘적 내용이 담겨 있습니다. 문서 데이터셋, 토크나이저(tokenizer), 자동 미분(autograd) 엔진, GPT-2 스타일의 신경망 아키텍처, Adam 옵티마이저, 학습 루프, 그리고 추론 루프까지 말이죠. 그 외의 모든 것은 단지 효율성을 위한 것일 뿐입니다. 저는 이보다 더 단순화할 수는 없습니다. 이 스크립트는 여러 프로젝트(micrograd, makemore, nanogpt 등)와 지난 10년 동안 LLM(거대언어모델)을 가장 핵심적인 요소로만 단순화하려 했던 제 집착의 정점이며, 저는 이것이 아름답다고 생각합니다 🥹. 심지어 3단 컬럼으로 완벽하게 나누어 떨어지기도 하죠.

### 어디서 볼 수 있나요:

*   이 GitHub gist에 전체 소스 코드가 있습니다: `microgpt.py`
*   이 웹페이지에서도 볼 수 있습니다: `https://karpathy.ai/microgpt.html`
*   Google Colab 노트북으로도 이용 가능합니다.

다음은 관심 있는 독자를 위해 코드를 단계별로 설명하는 가이드입니다.

---

### 데이터셋 (Dataset)
거대 언어 모델의 연료는 텍스트 데이터의 흐름이며, 선택적으로 여러 문서로 분리될 수 있습니다. 실제 상용 애플리케이션에서는 각 문서가 인터넷 웹페이지겠지만, microgpt에서는 더 간단한 예시로 한 줄에 하나씩 적힌 32,000개의 이름 데이터를 사용합니다:

```python
# 입력 데이터셋 `docs`가 있다고 가정: 문서들의 list[str] (예: 이름 데이터셋)
if not os.path.exists('input.txt'):
    import urllib.request
    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'
    urllib.request.urlretrieve(names_url, 'input.txt')
docs = [l.strip() for l in open('input.txt').read().strip().split('\n') if l.strip()] # 문서들의 리스트
random.shuffle(docs)
print(f"num docs: {len(docs)}")
```

데이터셋은 다음과 같습니다. 각 이름이 하나의 문서입니다:

```text
emma
olivia
ava
isabella
sophia
charlotte
mia
amelia
harper
... (약 32,000개의 이름이 이어짐)
```

모델의 목표는 데이터의 패턴을 학습한 다음, 그 안의 통계적 패턴을 공유하는 유사한 새 문서를 생성하는 것입니다. 미리 보기로 말씀드리자면, 스크립트가 끝날 때쯤 우리 모델은 그럴듯하게 들리는 새로운 이름들을 생성("환각" 또는 "할루시네이션"!)하게 될 것입니다. 결과를 미리 보면 다음과 같습니다:

```text
sample  1: kamon
sample  2: ann
sample  3: karai
sample  4: jaire
...
sample 20: anton
```

별것 아닌 것처럼 보일 수 있지만, ChatGPT 같은 모델의 관점에서는 여러분과의 대화도 그저 재미있게 생긴 "문서"일 뿐입니다. 프롬프트로 문서를 초기화하면, 모델의 관점에서 그에 대한 응답은 단지 통계적인 문서 완성 과정일 뿐입니다.

---

### 토크나이저 (Tokenizer)
내부적으로 신경망은 문자가 아닌 숫자로 작동하므로, 텍스트를 정수 토큰 ID 시퀀스로 변환하고 다시 되돌릴 방법이 필요합니다. GPT-4에서 사용하는 `tiktoken`과 같은 상용 토크나이저는 효율성을 위해 문자 덩어리(chunk) 단위로 작동하지만, 가장 단순한 토크나이저는 데이터셋의 각 고유 문자에 정수 하나를 할당하는 방식입니다:

```python
# 문자열을 이산 기호로 변환하고 다시 되돌리는 Tokenizer가 있다고 가정
uchars = sorted(set(''.join(docs))) # 데이터셋의 고유 문자들이 토큰 ID 0..n-1이 됨
BOS = len(uchars) # 시퀀스의 시작(Beginning of Sequence)을 알리는 특수 토큰 ID
vocab_size = len(uchars) + 1 # 고유 토큰의 총 개수, +1은 BOS를 위함
print(f"vocab size: {vocab_size}")
```

위 코드에서 우리는 데이터셋 전체의 모든 고유 문자를 수집하고(소문자 a-z가 됨), 정렬한 뒤, 각 문자에 인덱스로 ID를 부여합니다. 정수 값 자체에는 아무런 의미가 없습니다. 각 토큰은 서로 다른 별개의 이산 기호일 뿐입니다. 0, 1, 2 대신 서로 다른 이모티콘이어도 상관없습니다. 또한, 구분자 역할을 하는 **BOS**(Beginning of Sequence)라는 특수 토큰을 하나 더 만듭니다. 이는 모델에게 "여기서 새 문서가 시작/끝난다"라고 알려줍니다. 나중에 학습할 때 각 문서는 양쪽에 BOS로 감싸지게 됩니다: `[BOS, e, m, m, a, BOS]`. 모델은 BOS가 새로운 이름을 시작하고, 또 다른 BOS가 이름을 끝낸다는 것을 배웁니다. 따라서 우리는 최종적으로 27개의 어휘(26개의 소문자 a-z + BOS 토큰 1개)를 갖게 됩니다.

---

### 자동 미분 (Autograd)
신경망을 학습시키려면 기울기(gradient)가 필요합니다. 모델의 각 파라미터에 대해 "이 숫자를 조금 올리면 손실(loss)이 올라가나 내려가나? 그리고 얼마나?"를 알아야 합니다. 계산 그래프(computation graph)는 많은 입력(모델 파라미터와 입력 토큰)을 가지지만, 단 하나의 스칼라 출력인 **손실(Loss)**로 귀결됩니다(손실의 정의는 아래에서 합니다). 역전파(Backpropagation)는 그 단일 출력에서 시작하여 그래프를 거꾸로 거슬러 올라가며 모든 입력에 대한 손실의 기울기를 계산합니다. 이는 미적분학의 연쇄 법칙(chain rule)에 의존합니다. 실제 운영 환경에서는 PyTorch 같은 라이브러리가 이를 자동으로 처리합니다. 여기서는 `Value`라는 단일 클래스로 이를 처음부터 구현합니다.

```python
class Value:
    __slots__ = ('data', 'grad', '_children', '_local_grads')

    def __init__(self, data, children=(), local_grads=()):
        self.data = data                # 순전파(forward pass) 중 계산된 이 노드의 스칼라 값
        self.grad = 0                   # 역전파(backward pass) 중 계산된 이 노드에 대한 손실의 미분값
        self._children = children       # 계산 그래프에서 이 노드의 자식들
        self._local_grads = local_grads # 자식들에 대한 이 노드의 국소 미분값

    # ... (중략: __add__, __mul__ 등의 연산자 메서드들) ...

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        self.grad = 1
        for v in reversed(topo):
            for child, local_grad in zip(v._children, v._local_grads):
                child.grad += local_grad * v.grad
```

이 부분이 수학적으로나 알고리즘적으로 가장 강도가 높은 부분이라는 것을 알고 있으며, 이에 대한 2시간 반짜리 영상(micrograd video)도 있습니다. 간단히 말해, `Value`는 하나의 스칼라 숫자(`.data`)를 감싸고 그것이 어떻게 계산되었는지를 추적합니다. 각 연산을 작은 레고 블록이라고 생각하세요. 입력을 받아 출력을 생성하고(순전파), 각 입력에 대해 출력이 어떻게 변할지(국소 기울기)를 알고 있습니다. 이것이 오토그라드가 각 블록에서 필요로 하는 정보의 전부입니다. 그 외의 모든 것은 단지 블록들을 연결하는 연쇄 법칙일 뿐입니다.

`Value` 객체로 수학 연산(더하기, 곱하기 등)을 할 때마다, 입력(`_children`)과 그 연산의 국소 미분(`_local_grads`)을 기억하는 새로운 `Value`가 생성됩니다. 예를 들어, `__mul__`은 $\frac{\partial(a \cdot b)}{\partial a} = b$ 이고 $\frac{\partial(a \cdot b)}{\partial b} = a$ 임을 기록합니다.

레고 블록의 전체 목록:
*   (표 생략: 덧셈, 곱셈, 거듭제곱, 로그, 지수, ReLU 등의 연산과 그 미분값)

`backward()` 메서드는 이 그래프를 역 위상 정렬(reverse topological order) 순서로(손실에서 시작해 파라미터로 끝남) 걸어가며 각 단계에서 연쇄 법칙을 적용합니다. 손실이 $L$이고 노드 $v$가 자식 $c$를 가지며 국소 기울기가 $\frac{\partial v}{\partial c}$ 라면:

$$ \frac{\partial L}{\partial c} += \frac{\partial v}{\partial c} \cdot \frac{\partial L}{\partial v} $$

미적분에 익숙하지 않다면 무섭게 보일 수 있지만, 이건 말 그대로 두 숫자를 직관적인 방식으로 곱하는 것입니다. 이렇게 볼 수 있습니다: "자동차가 자전거보다 2배 빠르고 자전거가 걷는 사람보다 4배 빠르다면, 자동차는 걷는 사람보다 2 x 4 = 8배 빠르다." 연쇄 법칙은 경로를 따라 변화율을 곱하는 것과 같은 아이디어입니다.

우리는 손실 노드에서 `self.grad = 1`로 설정하며 시작합니다. 왜냐하면 $\frac{\partial L}{\partial L} = 1$, 즉 자기 자신에 대한 손실의 변화율은 당연히 1이기 때문입니다. 거기서부터 연쇄 법칙이 파라미터까지의 모든 경로를 따라 국소 기울기를 곱해 나갑니다.

`+=` (할당이 아닌 누적)에 주목하세요. 값이 그래프의 여러 곳에서 사용될 때(즉, 그래프가 분기할 때), 기울기는 각 분기를 따라 독립적으로 역류하므로 합산되어야 합니다. 이는 다변수 연쇄 법칙의 결과입니다.

`backward()`가 완료되면 그래프의 모든 `Value`는 $\frac{\partial L}{\partial v}$를 담고 있는 `.grad`를 갖게 되며, 이는 우리가 그 값을 아주 조금 움직였을 때 최종 손실이 어떻게 변할지를 알려줍니다.

PyTorch의 `.backward()`가 제공하는 것과 정확히 같습니다: (예시 코드 생략)

이것은 PyTorch의 `loss.backward()`가 실행하는 것과 알고리즘적으로 동일하며, 단지 텐서(스칼라의 배열) 대신 스칼라에 대해 실행될 뿐입니다. 알고리즘적으로는 같고 훨씬 작고 단순하지만, 물론 효율성은 훨씬 떨어집니다.

---

### 파라미터 (Parameters)
파라미터는 모델의 지식입니다. 이것들은 (autograd를 위해 `Value`로 감싸진) 수많은 부동 소수점 숫자들의 집합이며, 처음에는 무작위로 시작했다가 학습 중에 반복적으로 최적화됩니다. 각 파라미터의 정확한 역할은 아래에서 모델 아키텍처를 정의하면 더 이해가 되겠지만, 지금은 초기화만 하면 됩니다:

```python
# ... (파라미터 초기화 코드) ...
print(f"num params: {len(params)}")
```

각 파라미터는 가우스 분포에서 추출한 작은 난수로 초기화됩니다. `state_dict`는 이들을 이름이 있는 행렬로 정리합니다(PyTorch 용어를 빌려옴): 임베딩 테이블, 어텐션 가중치, MLP 가중치, 최종 출력 프로젝션 등입니다. 또한 나중에 옵티마이저가 반복 작업을 할 수 있도록 모든 파라미터를 하나의 리스트 `params`로 평탄화(flatten)합니다. 우리의 작은 모델에서는 이것이 4,192개의 파라미터가 됩니다. GPT-2는 16억 개였고, 최신 LLM들은 수천억 개에 달합니다.

---

### 아키텍처 (Architecture)
모델 아키텍처는 상태가 없는(stateless) 함수입니다. 토큰, 위치, 파라미터, 그리고 이전 위치에서 캐시된 키/값(Keys/Values)을 받아, 모델이 시퀀스에서 다음에 올 것이라고 생각하는 토큰에 대한 로짓(logits, 점수)을 반환합니다. 우리는 GPT-2를 따르되 소소하게 단순화했습니다: LayerNorm 대신 RMSNorm, 편향(bias) 제거, GeLU 대신 ReLU 사용 등입니다.

(세 가지 헬퍼 함수 `linear`, `softmax`, `rmsnorm` 설명 생략 - 본문 참조)

이제 모델 자체입니다:

```python
def gpt(token_id, pos_id, keys, values):
    # ... (임베딩, 어텐션 블록, MLP 블록, 출력 로짓 계산 코드) ...
    return logits
```

이 함수는 특정 시간 위치(`pos_id`)에 있는 하나의 토큰(`token_id`)과, `keys`와 `values`에 요약된 이전 반복의 맥락(KV Cache라고 알려짐)을 처리합니다. 단계별로 보면 다음과 같습니다:

1.  **임베딩 (Embeddings):** 신경망은 숫자 5 같은 원시 토큰 ID를 직접 처리할 수 없습니다. 벡터(숫자 리스트)로만 작업할 수 있습니다. 그래서 우리는 각 가능한 토큰에 학습된 벡터를 연결하고, 이를 신경 서명(neural signature)으로 입력합니다. 토큰 ID와 위치 ID는 각각의 임베딩 테이블(`wte`, `wpe`)에서 행을 조회합니다. 이 두 벡터가 더해져 모델에게 토큰이 무엇인지, 그리고 시퀀스의 어디에 있는지를 모두 인코딩한 표현을 제공합니다. (최신 LLM은 주로 위치 임베딩을 건너뛰고 RoPE 같은 상대적 위치 방식을 씁니다.)
2.  **어텐션 블록 (Attention block):** 현재 토큰은 쿼리(Q), 키(K), 값(V)이라는 세 개의 벡터로 투영됩니다. 직관적으로 쿼리는 "내가 무엇을 찾고 있는가?", 키는 "내가 무엇을 포함하고 있는가?", 값은 "내가 선택된다면 무엇을 제공하는가?"를 의미합니다. 예를 들어 "emma"라는 이름에서 두 번째 "m"에 있고 다음에 올 것을 예측하려 할 때, 모델은 "최근에 어떤 모음이 나왔지?" 같은 쿼리를 학습할 수 있습니다. 앞선 "e"는 이 쿼리에 잘 맞는 키를 가질 것이므로 높은 어텐션 가중치를 받고, 그 값(모음이라는 정보)이 현재 위치로 흘러들어옵니다. 키와 값은 KV 캐시에 추가되어 이전 위치들을 사용할 수 있게 됩니다. 어텐션은 **토큰 간의 소통 메커니즘**입니다.
3.  **MLP 블록:** MLP는 "다층 퍼셉트론(multilayer perceptron)"의 약자로, 2층짜리 피드포워드 네트워크입니다. 임베딩 차원을 4배로 늘리고, ReLU를 적용하고, 다시 줄입니다. 여기서 모델은 위치별로 대부분의 "생각"을 합니다. 어텐션과 달리 이 계산은 시간 $t$에 완전히 국소적입니다. 트랜스포머는 소통(어텐션)과 연산(MLP)을 번갈아 수행합니다.
4.  **잔차 연결 (Residual connections):** 어텐션과 MLP 블록 모두 출력을 입력에 다시 더합니다 (`x = [a + b for ...]`). 이는 기울기가 네트워크를 통해 직접 흐르게 하여 더 깊은 모델을 학습할 수 있게 합니다.
5.  **출력 (Output):** 마지막 은닉 상태(hidden state)는 `lm_head`에 의해 어휘 크기로 투영되어, 어휘 내 각 토큰당 하나의 로짓을 생성합니다. 우리 경우엔 27개의 숫자입니다. 로짓이 높을수록 모델은 해당 토큰이 다음에 올 확률이 높다고 생각하는 것입니다.

학습 중에 KV 캐시를 사용하는 것이 특이해 보일 수 있습니다. 보통 KV 캐시는 추론(inference)에만 쓴다고 생각하니까요. 하지만 KV 캐시는 개념적으로 항상 존재합니다. 상용 구현에서는 모든 위치를 동시에 처리하는 고도로 벡터화된 어텐션 계산 속에 숨겨져 있을 뿐입니다. microgpt는 한 번에 한 토큰씩 처리하므로(배치 차원 없음, 병렬 시간 단계 없음), KV 캐시를 명시적으로 구축합니다. 그리고 일반적인 추론 설정과 달리, 여기서 캐시된 키와 값은 계산 그래프 내의 살아있는 `Value` 노드이므로 실제로 그것들을 통해 역전파가 일어납니다.

---

### 학습 루프 (Training loop)
이제 모든 것을 연결합니다. 학습 루프는 반복적으로: (1) 문서를 고르고, (2) 토큰들에 대해 모델을 순전파 시키고, (3) 손실을 계산하고, (4) 역전파로 기울기를 얻고, (5) 파라미터를 업데이트합니다.

```python
# Adam 옵티마이저와 그 버퍼들이 있으라
# ...
# 순차적으로 반복
num_steps = 1000
for step in range(num_steps):
    # 문서 선택, 토큰화, BOS 추가
    # ...
    # 모델 순전파, 손실 계산
    # ...
    # 손실 역전파
    loss.backward()

    # Adam 옵티마이저 업데이트
    # ...
    print(f"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}")
```

각 부분을 살펴보겠습니다:

*   **토크나이징:** 각 학습 단계에서 문서 하나를 골라 양쪽에 BOS를 감쌉니다. "emma"는 `[BOS, e, m, m, a, BOS]`가 됩니다.
*   **순전파 및 손실:** 토큰을 하나씩 모델에 넣어 KV 캐시를 쌓아갑니다. 각 위치에서 모델은 27개의 로짓을 출력하고, 이를 소프트맥스를 통해 확률로 변환합니다. 손실은 정답 다음 토큰의 음의 로그 확률(negative log probability)인 $-\log p(\text{target})$입니다. 이를 **교차 엔트로피 손실(cross-entropy loss)**이라 합니다. 직관적으로, 손실은 모델이 실제 다음에 나온 것에 대해 얼마나 놀랐는지를 측정합니다. 문서 전체의 위치별 손실을 평균 내어 하나의 스칼라 손실을 얻습니다.
*   **역전파:** `loss.backward()` 한 번 호출로 전체 계산 그래프를 통해 역전파가 실행됩니다.
*   **Adam 옵티마이저:** 단순한 경사 하강법을 쓸 수도 있지만 Adam이 더 똑똑합니다. 파라미터당 두 개의 이동 평균(m: 모멘텀, v: 제곱 기울기의 평균)을 유지합니다. 1,000번의 스텝 동안 손실은 약 3.3(무작위 추측)에서 2.37 정도로 감소합니다. 숫자가 낮을수록 좋습니다.

---

### 추론 (Inference)
학습이 끝나면 모델에서 새로운 이름을 샘플링할 수 있습니다. 파라미터는 고정(frozen)되고, 순전파를 루프 돌리면서 생성된 각 토큰을 다음 입력으로 다시 넣습니다.

```python
# ... (추론 코드) ...
```

각 샘플을 BOS 토큰으로 시작합니다. 모델은 27개의 로짓을 생성하고, 우리는 이를 확률로 변환한 뒤 확률에 따라 토큰 하나를 무작위로 샘플링합니다. 그 토큰은 다시 다음 입력으로 들어갑니다. 모델이 다시 BOS를 생성하거나 최대 길이에 도달할 때까지 반복합니다.

`temperature`(온도) 파라미터는 무작위성을 제어합니다. 낮으면(0.5 등) 모델이 보수적으로 변해 상위 선택지를 주로 고르고, 1.0이면 학습된 분포 그대로, 0에 가까우면 가장 확률 높은 것만 고릅니다(greedy decoding).

---

### 실행 (Run it)
파이썬만 있으면 됩니다 (pip install 필요 없음, 의존성 없음):

`python train.py`

제 맥북에서는 약 1분 정도 걸립니다. 손실이 ~3.3에서 ~2.37로 내려가는 것을 지켜보세요. 학습이 끝나면 통계적 패턴이 파라미터에 증류되어 저장됩니다. 이를 통해 새롭고 '환각된' 이름들을 생성할 수 있습니다.

(샘플 출력 생략)

---

### 발전 과정 (Progression)
양파 껍질을 벗기듯 코드가 쌓여가는 과정을 보려면 다음 순서를 권장합니다:

*   `train0.py`: 바이그램(Bigram) 카운트 테이블 — 신경망 없음, 기울기 없음
*   `train1.py`: MLP + 수동 기울기 계산 (수치적 & 해석적) + SGD
*   `train2.py`: Autograd (Value 클래스) — 수동 기울기 대체
*   `train3.py`: 위치 임베딩 + 단일 헤드 어텐션 + rmsnorm + 잔차 연결
*   `train4.py`: 멀티 헤드 어텐션 + 레이어 루프 — 완전한 GPT 아키텍처
*   `train5.py`: Adam 옵티마이저 — 이것이 `train.py`입니다

`build_microgpt.py`라는 Gist를 만들어 두었으니 리비전(Revisions)을 통해 각 단계의 차이(diff)를 확인해 보세요.

---

### 실제와의 차이 (Real stuff)
microgpt는 GPT 학습 및 실행의 알고리즘적 본질을 모두 담고 있습니다. 하지만 이것과 ChatGPT 같은 상용 LLM 사이에는 많은 차이점이 있습니다. 핵심 알고리즘은 같지만, 규모 확장을 위해 필요한 것들입니다:

*   **데이터:** 32K의 이름 대신 수조 개의 토큰(웹페이지, 책, 코드 등).
*   **토크나이저:** 문자 대신 BPE(Byte Pair Encoding) 같은 서브워드(subword) 토크나이저 사용. 어휘 크기는 약 10만 개.
*   **Autograd:** 파이썬 스칼라 객체 대신 텐서(Tensor)를 사용하며 GPU/TPU에서 실행. PyTorch와 CUDA 커널(FlashAttention 등) 사용. 수학은 동일하지만 병렬 처리됨.
*   **아키텍처:** microgpt는 4,192 파라미터, GPT-4 급은 수천억 개. RoPE, GQA, MoE 등 최신 기법 사용.
*   **학습:** 배치 처리, 기울기 누적, 혼합 정밀도(float16/bfloat16). 수천 개의 GPU로 몇 달간 학습.
*   **최적화:** 대규모에서는 최적화 자체가 하나의 학문입니다. 하이퍼파라미터 튜닝과 스케일링 법칙(Scaling laws)이 중요합니다.
*   **사후 학습 (Post-training):** 학습된 베이스 모델은 문서 완성기일 뿐입니다. ChatGPT가 되려면 **SFT**(지도 미세 조정)와 **RL**(강화 학습) 단계를 거쳐야 합니다.
*   **추론:** KV 캐시 관리(vLLM 등), 양자화(quantization) 등 엔지니어링이 많이 들어갑니다.

이 모든 것이 중요한 엔지니어링 및 연구 기여지만, microgpt를 이해한다면 알고리즘적 본질은 이해한 것입니다.

---

### 자주 묻는 질문 (FAQ)
**모델이 무언가를 "이해"하나요?** 철학적인 질문입니다만, 기계적으로는 아닙니다. 마법은 없습니다. 모델은 입력 토큰을 다음 토큰의 확률 분포로 매핑하는 거대한 수학 함수일 뿐입니다.

**왜 작동하나요?** 수천 개의 파라미터가 최적화 과정을 통해 데이터의 통계적 규칙성을 포착하는 값으로 조정되기 때문입니다. (예: 이름은 자음으로 시작하는 경향이 있다 등).

**ChatGPT와는 무슨 관련인가요?** ChatGPT는 이 핵심 루프를 엄청나게 키우고, 대화가 가능하도록 사후 학습을 거친 것입니다.

**"환각"은 왜 생기나요?** 모델은 확률 분포에서 샘플링하여 토큰을 생성합니다. 진실에 대한 개념이 없고, 학습 데이터상 무엇이 통계적으로 그럴듯한지만 압니다. microgpt가 가짜 이름을 만들어내는 현상은 ChatGPT가 틀린 사실을 자신 있게 말하는 것과 같은 현상입니다.

**왜 이렇게 느린가요?** 순수 파이썬으로 스칼라를 하나씩 처리하기 때문입니다. GPU는 수백만 개의 스칼라를 병렬로 처리합니다.
